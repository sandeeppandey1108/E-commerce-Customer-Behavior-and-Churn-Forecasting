{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install Libraries and Import Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, install the necessary Python libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy seaborn matplotlib scikit-learn statsmodels keras pmdarima catboost lightgbm xgboost transformers dask[dataframe] imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import all the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pmdarima import auto_arima\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load the Datasets\n",
    "Specify the paths and load your datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Datasets\n",
    "large_data_path = r\"C:\\Users\\sande\\Music\\BI\\Data_Set\\ecommerce_customer_data_large.csv\"\n",
    "custom_ratios_path = r\"C:\\Users\\sande\\Music\\BI\\Data_Set\\ecommerce_customer_data_custom_ratios.csv\"\n",
    "\n",
    "df_large = pd.read_csv(large_data_path)\n",
    "df_custom = pd.read_csv(custom_ratios_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Concatenate the Datasets\n",
    "Merge the two datasets into one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Datasets\n",
    "df_combined = pd.concat([df_large, df_custom])\n",
    "df_combined.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Exploratory Data Analysis (EDA)\n",
    "You can explore the combined dataset to understand its structure and contents:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in combined dataset:\\n\", df_combined.isnull().sum())\n",
    "\n",
    "# Summary statistics of numerical columns\n",
    "print(df_combined.describe())\n",
    "\n",
    "# Impute missing values in 'Returns' and 'Age'\n",
    "df_combined['Returns'].fillna(0, inplace=True)\n",
    "df_combined['Age'].fillna(df_combined['Age'].mean(), inplace=True)\n",
    "\n",
    "# Visualize the distribution of purchase amounts\n",
    "sns.histplot(df_combined['Total Purchase Amount'], bins=20)\n",
    "plt.title(\"Total Purchase Amount Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Check the correlation between numerical features\n",
    "numeric_df = df_combined.select_dtypes(include=[np.number])  # Only numeric columns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Data Preprocessing\n",
    "Drop irrelevant columns, handle dates, and prepare data for modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "columns_to_drop = ['Customer Name', 'Customer ID']\n",
    "df_combined = df_combined.drop(columns=[col for col in columns_to_drop if col in df_combined.columns])\n",
    "\n",
    "# Handle 'Purchase Date' column\n",
    "if 'Purchase Date' in df_combined.columns:\n",
    "    df_combined['Purchase Date'] = pd.to_datetime(df_combined['Purchase Date'])\n",
    "    df_combined.set_index('Purchase Date', inplace=True)\n",
    "\n",
    "# Print columns to check\n",
    "print(\"Columns after preprocessing:\", df_combined.columns.tolist())\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "categorical_columns = ['Product Category', 'Payment Method', 'Gender']\n",
    "existing_categorical_columns = [col for col in categorical_columns if col in df_combined.columns]\n",
    "df_combined = pd.get_dummies(df_combined, columns=existing_categorical_columns, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Split Features and Target\n",
    "Define the target variable and split the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = df_combined.drop('Churn', axis=1)\n",
    "y = df_combined['Churn']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "numeric_cols = ['Product Price', 'Quantity', 'Total Purchase Amount', 'Age']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Handle Class Imbalance using SMOTE\n",
    "Balance the dataset using SMOTE to handle imbalanced classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Train and Evaluate Models\n",
    "Train multiple models and evaluate their performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight='balanced', max_iter=300, solver='liblinear'),\n",
    "    \"RandomForest\": RandomForestClassifier(class_weight='balanced'),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=False),\n",
    "    \"LightGBM\": LGBMClassifier(),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"\\nModel Evaluation:\\nAccuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    print(f\"{name} trained successfully.\")\n",
    "    evaluate_model(model, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Time Series Forecasting (ARIMA & LSTM)\n",
    "You can forecast future values using ARIMA and LSTM models:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series aggregation by month\n",
    "df_time_series = df_combined['Total Purchase Amount'].resample('M').sum().dropna()\n",
    "\n",
    "# ARIMA Model\n",
    "model_arima = auto_arima(df_time_series, seasonal=True, stepwise=True, suppress_warnings=True)\n",
    "forecast_arima = model_arima.predict(n_periods=12)  # Forecast for the next 12 months\n",
    "print(\"ARIMA Forecast:\", forecast_arima)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "def prepare_lstm_data(data, time_step=30):\n",
    "    data = data.values\n",
    "    X, y = [], []\n",
    "    for i in range(time_step, len(data)):\n",
    "        X.append(data[i-time_step:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_lstm, y_lstm = prepare_lstm_data(df_time_series)\n",
    "X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 1))\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, return_sequences=True, input_shape=(X_lstm.shape[1], 1)))\n",
    "lstm_model.add(LSTM(50))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(X_lstm, y_lstm, epochs=50, batch_size=32)\n",
    "\n",
    "# Forecast the next month\n",
    "lstm_input = df_time_series.values[-30:].reshape((1, 30, 1))\n",
    "lstm_prediction = lstm_model.predict(lstm_input)\n",
    "print(\"LSTM Forecast:\", lstm_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Compare ARIMA and LSTM Forecasts\n",
    "Visualize and compare the forecasts from ARIMA and LSTM:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ARIMA and LSTM forecasts\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_time_series, label='Historical Data', color='blue')\n",
    "plt.plot(pd.date_range(start='2023-10-31', periods=12, freq='M'), forecast_arima, label='ARIMA Forecast', color='orange')\n",
    "plt.plot(pd.date_range(start='2023-10-31', periods=1, freq='M'), lstm_prediction.flatten(), label='LSTM Forecast', color='green')\n",
    "plt.title('Forecast Comparison')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Purchase Amount')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
